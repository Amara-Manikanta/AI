{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d1d369",
   "metadata": {},
   "source": [
    "<img src=\"https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png\" class=\"logo\" width=\"120\"/>\n",
    "\n",
    "## Local Personalised Chatbot on MacBook Pro: Feasibility, Models, and Guide\n",
    "\n",
    "### Is This Feasible?\n",
    "\n",
    "**Yes**—running a personalized offline chatbot on your MacBook Pro is not just feasible but increasingly popular. Thanks to efficient open-source models and easy-to-use frameworks, you can download and run highly capable large language models (LLMs) completely offline, ensuring privacy and control. Your device should have at least 8GB of RAM for decent performance, and more RAM will allow you to use larger, more powerful models[^1_1][^1_2][^1_3].\n",
    "\n",
    "### Recommended Model \\& Frameworks\n",
    "\n",
    "For your use case, consider these options:\n",
    "\n",
    "- **Llama 2/3 (Meta)**: Capable and widely supported, available in quantized formats for speed and efficiency.\n",
    "- **Mistral 7B**: A lightweight, high-performance alternative.\n",
    "- **Ollama**: User-friendly application to download/run these models locally, with easy setup for Mac[^1_4][^1_3][^1_2].\n",
    "- **llama-cpp-python**: Python library for Llama models, supports detailed memory management and pure offline workflows[^1_3][^1_5].\n",
    "\n",
    "**Model Memory Capability:**\n",
    "No local LLM “remembers” you between sessions out-of-the-box. However, you can implement a \"memory\" in Python—saving important facts or chat histories to files and reloading them, so the chatbot can recall personal details (e.g. your name, preferences)[^1_3][^1_6].\n",
    "\n",
    "### Step-by-Step Setup\n",
    "\n",
    "#### 1. Install Prerequisites\n",
    "\n",
    "- **Python 3.9+** (using Homebrew or official download)\n",
    "- **Homebrew** (if not already):\n",
    "`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n",
    "- **Ollama**:\n",
    "Download and install from the official site[^1_4].\n",
    "    - Drag the app to Applications and open it; follow on-screen instructions.\n",
    "    - Optionally, install via Homebrew: `brew install ollama`\n",
    "- Alternatively (for more customizability):\n",
    "`pip install llama-cpp-python`[^1_3].\n",
    "\n",
    "\n",
    "#### 2. Download a Model\n",
    "\n",
    "In your terminal:\n",
    "\n",
    "- **Ollama:**\n",
    "Run `ollama run llama3`\n",
    "(downloads and sets up Meta Llama 3 automatically)\n",
    "- **llama-cpp-python (manual):**\n",
    "Download a quantized model file (GGUF format)—for example, Mistral 7B—from HuggingFace.\n",
    "Example:\n",
    "`wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O model.gguf`[^1_3][^1_2].\n",
    "\n",
    "\n",
    "#### 3. Python Script for a Personalized Offline Chatbot\n",
    "\n",
    "Below is a basic script using `llama-cpp-python` that:\n",
    "\n",
    "- Loads a local model\n",
    "- Allows interactive chatting\n",
    "- Remembers and saves personal details (e.g. your name, favorite things)\n",
    "- Stores \"facts\" in a file, so the model can recall them in every session\n",
    "\n",
    "```python\n",
    "import llama_cpp\n",
    "import json\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"model.gguf\"   # Adjust if your model file has a different name\n",
    "HISTORY_FILE = \"chat_history.json\"\n",
    "FACTS_FILE = \"user_facts.json\"\n",
    "\n",
    "# Load or initialize memory\n",
    "def load_memory(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def save_memory(file_path, data):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "conversation_history = load_memory(HISTORY_FILE)\n",
    "important_facts = load_memory(FACTS_FILE)\n",
    "\n",
    "# Simple fact detection (customizable)\n",
    "def detect_fact(user_input):\n",
    "    tags = [\"my name is\", \"i live in\", \"my favorite\", \"i work as\"]\n",
    "    for tag in tags:\n",
    "        if tag in user_input.lower():\n",
    "            return user_input\n",
    "    return None\n",
    "\n",
    "def process_personal_query(user_input):\n",
    "    if \"what's my name\" in user_input:\n",
    "        for fact in important_facts:\n",
    "            if \"name is\" in fact.lower():\n",
    "                return fact.replace(\"my name is\", \"Your name is\")\n",
    "        return \"I don't know your name yet. Please tell me by saying 'My name is ...'.\"\n",
    "    return None\n",
    "\n",
    "llm = llama_cpp.Llama(model_path=MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "def chatbot():\n",
    "    global conversation_history, important_facts\n",
    "    print(\"AI Assistant: How can I help you? (type 'exit' to quit)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            save_memory(HISTORY_FILE, conversation_history)\n",
    "            save_memory(FACTS_FILE, important_facts)\n",
    "            print(\"AI Assistant: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Remember facts\n",
    "        fact = detect_fact(user_input)\n",
    "        if fact and fact not in important_facts:\n",
    "            important_facts.append(fact)\n",
    "            save_memory(FACTS_FILE, important_facts)\n",
    "            print(\"AI Assistant: Got it, I'll remember that!\")\n",
    "            continue\n",
    "\n",
    "        # Respond to personal queries from memory\n",
    "        response = process_personal_query(user_input)\n",
    "        if response:\n",
    "            print(f\"AI Assistant: {response}\")\n",
    "            continue\n",
    "\n",
    "        # Maintain short-term context\n",
    "        conversation_history.append(f\"User: {user_input}\")\n",
    "        if len(conversation_history) > 10:  # Keep last 10 exchanges\n",
    "            conversation_history.pop(0)\n",
    "        prompt = \"\\n\".join(important_facts) + \"\\n\" + \"\\n\".join(conversation_history) + \"\\nAI Assistant:\"\n",
    "\n",
    "        # Generate response\n",
    "        out = llm(prompt, max_tokens=200)\n",
    "        assistant_reply = out['choices'][^1_0]['text'].strip()\n",
    "        conversation_history.append(f\"AI Assistant: {assistant_reply}\")\n",
    "        print(f\"AI Assistant: {assistant_reply}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n",
    "```\n",
    "\n",
    "\n",
    "#### 4. Run the Script\n",
    "\n",
    "- Open a terminal in your script directory.\n",
    "- Activate your Python virtual environment, if you use one.\n",
    "- Run:\n",
    "`python your_script_name.py`\n",
    "\n",
    "\n",
    "### Additional Customization\n",
    "\n",
    "- Expand the `detect_fact()` rules for more personalized details.\n",
    "- Add a simple web UI with Flask or Streamlit if you want a graphical interface.\n",
    "- Scale up/down model size depending on RAM and CPU (smaller models = faster, larger models = smarter)[^1_1][^1_3].\n",
    "\n",
    "\n",
    "### Final Notes\n",
    "\n",
    "- **Local chatbots are secure and private**: Data and chat never leave your MacBook[^1_2][^1_3].\n",
    "- **Model quality**: For most everyday conversation, open-source models like Llama, Mistral, or similar are now very capable offline—though not quite on par with large cloud models like GPT-4[^1_1][^1_3].\n",
    "- **Memory and persistence**: With the provided pattern, your chatbot can \"remember\" any personal fact you wish—simply extend the fact detection and retrieval logic to match your needs[^1_3][^1_6].\n",
    "\n",
    "You can always further tune, finetune, or adjust the memory mechanism for deeper personalization.\n",
    "\n",
    "**References:**\n",
    "[^1_1], [^1_2], [^1_4], [^1_5], [^1_3]\n",
    "\n",
    "<div style=\"text-align: center\">⁂</div>\n",
    "\n",
    "[^1_1]: https://dev.to/best_codes/5-best-ai-models-you-can-run-locally-on-your-device-475h\n",
    "\n",
    "[^1_2]: https://rigorousthemes.com/blog/private-ai-chatbots-you-can-run-locally/\n",
    "\n",
    "[^1_3]: https://ruan.dev/blog/2025/02/14/building-a-local-ai-assistant-with-llama-cpp-python\n",
    "\n",
    "[^1_4]: https://ollama.com/download/mac\n",
    "\n",
    "[^1_5]: https://pyimagesearch.com/2024/08/26/llama-cpp-the-ultimate-guide-to-efficient-llm-inference-and-applications/\n",
    "\n",
    "[^1_6]: https://hackernoon.com/chatbot-memory-implement-your-own-algorithm-from-scratch\n",
    "\n",
    "[^1_7]: https://apps.apple.com/pl/app/offline-chatbot-private-ai/id6657958995\n",
    "\n",
    "[^1_8]: https://www.reddit.com/r/macapps/comments/16fy20u/private_llm_a_gpt_chatbot_that_runs_fully_offline/\n",
    "\n",
    "[^1_9]: https://jan.ai\n",
    "\n",
    "[^1_10]: https://privatellm.app/en\n",
    "\n",
    "[^1_11]: https://www.youtube.com/watch?v=uDOHLshaPWc\n",
    "\n",
    "[^1_12]: https://github.com/alphaolomi/local-ai-chatbot\n",
    "\n",
    "[^1_13]: https://github.com/opsec24/llama_chatbot\n",
    "\n",
    "[^1_14]: https://www.youtube.com/watch?v=SrB0Z6MS5KQ\\&vl=en-US\n",
    "\n",
    "[^1_15]: https://www.youtube.com/watch?v=G4XdtuwItAc\n",
    "\n",
    "[^1_16]: https://alternativeto.net/software/offlinellm/\n",
    "\n",
    "[^1_17]: https://www.tomsguide.com/ai/you-can-run-your-own-ai-chatbot-locally-on-windows-and-mac-heres-how\n",
    "\n",
    "[^1_18]: https://www.toolify.ai/ai-news/unleash-the-power-of-local-language-models-offline-chat-gpt-at-your-fingertips-872516\n",
    "\n",
    "[^1_19]: https://www.pcmag.com/how-to/how-to-run-your-own-chatgpt-like-llm-for-free-and-in-private\n",
    "\n",
    "[^1_20]: https://topai.tools/s/offline-language-model\n",
    "\n",
    "[^1_21]: https://github.com/getumbrel/llama-gpt\n",
    "\n",
    "[^1_22]: https://huggingface.co/Jasleen05/my-local-chatbot\n",
    "\n",
    "[^1_23]: https://www.reddit.com/r/selfhosted/comments/15hk9d2/is_there_a_list_of_all_usable_ai_models_that_can/\n",
    "\n",
    "[^1_24]: https://www.nomic.ai/gpt4all\n",
    "\n",
    "[^1_25]: https://www.youtube.com/watch?v=d0o89z134CQ\n",
    "\n",
    "[^1_26]: https://realpython.com/build-llm-rag-chatbot-with-langchain/\n",
    "\n",
    "[^1_27]: https://github.com/iSiddharth20/LLM-Chatbot\n",
    "\n",
    "[^1_28]: https://web.dev/articles/ai-chatbot-webllm\n",
    "\n",
    "[^1_29]: https://ai.plainenglish.io/how-i-built-a-local-first-ai-chatbot-that-works-offline-and-understands-my-files-0c46c4441870\n",
    "\n",
    "[^1_30]: https://github.com/lcary/local-chatgpt-app\n",
    "\n",
    "[^1_31]: https://itnext.io/remembering-conversations-building-chatbots-with-short-and-long-term-memory-on-aws-c1361c130046?gi=03179b105215\n",
    "\n",
    "[^1_32]: https://github.com/Nazakun021/local-llm-chatbot\n",
    "\n",
    "[^1_33]: https://dev.to/mehmetakar/5-ways-to-run-llm-locally-on-mac-cck\n",
    "\n",
    "[^1_34]: https://peterfalkingham.com/2024/04/26/my-experience-training-a-local-llm-ai-chatbot-on-local-data/\n",
    "\n",
    "[^1_35]: https://boltai.com/blog/run-llm-locally-on-mac\n",
    "\n",
    "[^1_36]: https://ijrpr.com/uploads/V6ISSUE6/IJRPR49341.pdf\n",
    "\n",
    "[^1_37]: https://www.reddit.com/r/LocalLLaMA/comments/13vhev0/introducing_localgpt_offline_chatbot_for_your/\n",
    "\n",
    "[^1_38]: https://www.youtube.com/watch?v=e5iaYkSNrhY\n",
    "\n",
    "[^1_39]: https://chattube.io/summary/science-technology/Coj72EzmX20\n",
    "\n",
    "[^1_40]: https://www.toolify.ai/ai-news/local-llms-run-large-language-models-offline-3315817\n",
    "\n",
    "[^1_41]: https://mahdisguide.com/chatbot-offline-capabilities/\n",
    "\n",
    "[^1_42]: https://dev.to/up_min_sparcs/how-to-make-a-chatbot-in-python-using-a-local-llm-7h8\n",
    "\n",
    "[^1_43]: https://www.metriccoders.com/post/how-to-install-and-run-ollama-on-macos\n",
    "\n",
    "[^1_44]: https://gptforwork.com/help/ai-models/custom-endpoints/set-up-ollama-on-macos\n",
    "\n",
    "[^1_45]: https://github.com/ollama/ollama\n",
    "\n",
    "[^1_46]: https://www.youtube.com/watch?v=JIEnITPL9ck\n",
    "\n",
    "[^1_47]: https://translucentcomputing.github.io/kubert-assistant-lite/ollama.html\n",
    "\n",
    "[^1_48]: https://www.youtube.com/watch?v=3V-EBWZl2PE\n",
    "\n",
    "[^1_49]: https://dev.to/danielbayerlein/run-llms-locally-with-ollama-on-macos-for-developers-5emb\n",
    "\n",
    "[^1_50]: https://dev.to/ishaa_twt/building-llamacpp-based-local-ai-chat-assistant-4ip4\n",
    "\n",
    "[^1_51]: https://www.vellum.ai/blog/how-should-i-manage-memory-for-my-llm-chatbot\n",
    "\n",
    "[^1_52]: https://www.youtube.com/watch?v=oI7VoTM9NKQ\n",
    "\n",
    "[^1_53]: https://www.youtube.com/watch?v=rStOK2FfyEY\n",
    "\n",
    "[^1_54]: https://python.langchain.com/docs/how_to/chatbots_memory/\n",
    "\n",
    "[^1_55]: https://blog.stackademic.com/installing-and-using-ollama-on-macos-acabe185fa3a?gi=5c8f7140a353\n",
    "\n",
    "[^1_56]: https://www.youtube.com/watch?v=YDj_ScvBpKU\n",
    "\n",
    "[^1_57]: https://blog.devops.dev/local-gen-ai-chatbot-with-memory-using-ollama-llama3-using-python-3e07f4057cad\n",
    "\n",
    "[^1_58]: https://dev.to/mitchallen/ollama-getting-started-llama-3-mac-apple-silicon-39kn\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Guide: Creating a Personalized Offline Chatbot Desktop App on MacBook Pro\n",
    "\n",
    "Below is a detailed process you can follow to set up your own personalized offline chatbot as both a Python notebook (ipynb) and a desktop application for your MacBook Pro. Each step is broken into actionable tasks—with clear instructions and example code to help you at each stage.\n",
    "\n",
    "### 1. Prerequisites\n",
    "\n",
    "- **Operating System**: macOS (MacBook Pro)\n",
    "- **Python**: Version 3.9 or newer (preferably 3.10+)\n",
    "- **Homebrew**: For easy package management\n",
    "- **Model Format**: GGUF for efficient offline usage\n",
    "\n",
    "\n",
    "#### Install Python and Homebrew\n",
    "\n",
    "Open your terminal and run:\n",
    "\n",
    "```bash\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "brew install python\n",
    "```\n",
    "\n",
    "\n",
    "### 2. Download the Language Model and Required Packages\n",
    "\n",
    "We'll use the `llama-cpp-python` library and the Mistral 7B Instruct model as an example. Install the Python dependencies:\n",
    "\n",
    "```bash\n",
    "pip install llama-cpp-python==0.2.72 gradio==4.29.0\n",
    "pip install jupyterlab\n",
    "```\n",
    "\n",
    "\n",
    "#### Download a Quantized Model\n",
    "\n",
    "Visit [TheBloke’s GGUF models on Hugging Face](https://huggingface.co/TheBloke) and download, for example, Mistral-7B–Instruct (GGUF Q4_K_M).\n",
    "\n",
    "In your terminal:\n",
    "\n",
    "```bash\n",
    "mkdir ~/local_chatbot\n",
    "cd ~/local_chatbot\n",
    "wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O model.gguf\n",
    "```\n",
    "\n",
    "\n",
    "### 3. Prepare the Jupyter Notebook (`.ipynb`) Structure\n",
    "\n",
    "**Create a new notebook in JupyterLab:**\n",
    "\n",
    "```bash\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "Add the following cells step by step.\n",
    "\n",
    "#### [Markdown Cell] Introduction\n",
    "\n",
    "```markdown\n",
    "# Personalized Offline Chatbot (Desktop App Style)\n",
    "\n",
    "This notebook helps you build and test your own offline chatbot on your MacBook Pro using Python, `llama-cpp-python`, and a local GGUF model. Later, you'll package it as a simple desktop application.\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Import Libraries \\& Set Constants\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "from llama_cpp import Llama\n",
    "\n",
    "MODEL_PATH = \"model.gguf\"\n",
    "HISTORY_FILE = \"chat_history.json\"\n",
    "FACTS_FILE = \"user_facts.json\"\n",
    "```\n",
    "\n",
    "\n",
    "#### [Markdown Cell] Model Download Check\n",
    "\n",
    "```markdown\n",
    "## Confirm Model Download\n",
    "\n",
    "Ensure you have downloaded your GGUF model (`model.gguf`) and placed it in the notebook directory.\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Helper Functions: Memory\n",
    "\n",
    "```python\n",
    "def load_memory(file_path, default):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return default\n",
    "\n",
    "def save_memory(file_path, data):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Define Fact Parsing and Personal Query Handling\n",
    "\n",
    "```python\n",
    "def detect_fact(user_input):\n",
    "    tags = [\"my name is\", \"i live in\", \"my favorite\", \"i work as\"]\n",
    "    for tag in tags:\n",
    "        if tag in user_input.lower():\n",
    "            return user_input\n",
    "    return None\n",
    "\n",
    "def process_personal_query(user_input, facts):\n",
    "    if \"what's my name\" in user_input:\n",
    "        for fact in facts:\n",
    "            if \"name is\" in fact.lower():\n",
    "                return fact.replace(\"my name is\", \"Your name is\")\n",
    "        return \"I don't know your name yet.\"\n",
    "    return None\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Load Model\n",
    "\n",
    "```python\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=2048)\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Main Chat Function for Notebook\n",
    "\n",
    "```python\n",
    "conversation_history = load_memory(HISTORY_FILE, [])\n",
    "important_facts = load_memory(FACTS_FILE, [])\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    out = llm(prompt, max_tokens=200)\n",
    "    return out['choices'][0]['text'].strip()\n",
    "\n",
    "def add_message(role, message, history):\n",
    "    history.append(f\"{role}: {message}\")\n",
    "    if len(history) > 10:  # Maintain recent history\n",
    "        history.pop(0)\n",
    "    return history\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Chat Loop (for Interactive Notebook Use)\n",
    "\n",
    "```python\n",
    "def chatbot():\n",
    "    print(\"AI Assistant: How can I help you? (type 'exit' to quit)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower().strip() == 'exit':\n",
    "            save_memory(HISTORY_FILE, conversation_history)\n",
    "            save_memory(FACTS_FILE, important_facts)\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Store facts\n",
    "        fact = detect_fact(user_input)\n",
    "        if fact and fact not in important_facts:\n",
    "            important_facts.append(fact)\n",
    "            save_memory(FACTS_FILE, important_facts)\n",
    "            print(\"AI Assistant: Got it—I'll remember that!\")\n",
    "            continue\n",
    "\n",
    "        # Personal info queries\n",
    "        resp = process_personal_query(user_input, important_facts)\n",
    "        if resp:\n",
    "            print(f\"AI Assistant: {resp}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare prompt and respond\n",
    "        prompt = \"\\n\".join(important_facts) + \"\\n\" + \"\\n\".join(conversation_history) + f\"\\nUser: {user_input}\\nAI Assistant:\"\n",
    "        assistant_reply = get_llm_response(prompt)\n",
    "        add_message(\"User\", user_input, conversation_history)\n",
    "        add_message(\"AI Assistant\", assistant_reply, conversation_history)\n",
    "        print(f\"AI Assistant: {assistant_reply}\")\n",
    "```\n",
    "\n",
    "\n",
    "#### [Markdown Cell] Run Your Chatbot\n",
    "\n",
    "```markdown\n",
    "### Start Chat in Notebook\n",
    "To begin chatting, run the following cell:\n",
    "```\n",
    "\n",
    "\n",
    "#### [Code Cell] Start Interaction\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n",
    "```\n",
    "\n",
    "\n",
    "### 4. Desktop Application Version with Gradio\n",
    "\n",
    "You can use Gradio to package your chatbot in a simple desktop web app (runs locally, accessed via browser window).\n",
    "\n",
    "#### [Code Cell] Gradio Desktop App\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "conversation_history = load_memory(HISTORY_FILE, [])\n",
    "important_facts = load_memory(FACTS_FILE, [])\n",
    "\n",
    "def gradio_chat(user_input, history):\n",
    "    fact = detect_fact(user_input)\n",
    "    response = \"\"\n",
    "    if fact and fact not in important_facts:\n",
    "        important_facts.append(fact)\n",
    "        save_memory(FACTS_FILE, important_facts)\n",
    "        response = \"Got it—I'll remember that!\"\n",
    "    else:\n",
    "        resp = process_personal_query(user_input, important_facts)\n",
    "        if resp:\n",
    "            response = resp\n",
    "        else:\n",
    "            prompt = \"\\n\".join(important_facts) + \"\\n\" + \"\\n\".join([f\"User: {u}\\nAI Assistant: {a}\" for u,a in history[-5:]]) + f\"\\nUser: {user_input}\\nAI Assistant:\"\n",
    "            response = get_llm_response(prompt)\n",
    "    history.append((user_input, response))\n",
    "    add_message(\"User\", user_input, conversation_history)\n",
    "    add_message(\"AI Assistant\", response, conversation_history)\n",
    "    save_memory(HISTORY_FILE, conversation_history)\n",
    "    return history, history\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    gradio_chat,\n",
    "    chatbot=gr.Chatbot(height=400),\n",
    "    title=\"Local Personalized Chatbot\"\n",
    ")\n",
    "\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "- When you run this cell, your default browser will open to a chat interface.\n",
    "- You can package this with tools like `pyinstaller` or `briefcase` as a standalone app if desired.\n",
    "\n",
    "\n",
    "### 5. Optional: Pack as a Desktop Application\n",
    "\n",
    "For a native app, use [PyInstaller](https://pyinstaller.org/):\n",
    "\n",
    "```bash\n",
    "pip install pyinstaller\n",
    "pyinstaller --onefile your_script_name.py\n",
    "```\n",
    "\n",
    "This creates a standalone `.app` or binary for your Mac in the `dist/` directory.\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Step | What You Do |\n",
    "| :-- | :-- |\n",
    "| Install requirements | Python, Homebrew, necessary packages |\n",
    "| Download the model | Fetch GGUF model and place in your project directory |\n",
    "| Build notebook | Step-by-step, input code cells into JupyterLab |\n",
    "| Personalize chatbot | Extend facts memory, tweak detection as you wish |\n",
    "| Desktop app version | Run Gradio UI cell, optionally bundle using PyInstaller |\n",
    "\n",
    "Your chatbot will now run entirely offline, on your MacBook Pro, with persistent and growing memory for personalized experiences. Adjust the fact detection and interface as you grow more comfortable with the platform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c390a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
