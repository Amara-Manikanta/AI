{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic Analysis in Natural Language Processing (NLP)\n",
    "\n",
    "## 1. Lexical Processing\n",
    "\n",
    "#### Description:\n",
    "Lexical processing involves analyzing individual words in a text. It focuses on recognizing and understanding words as discrete units of meaning (lexemes), identifying their properties, such as part of speech, base forms, and inflections.\n",
    "\n",
    "#### Key Tasks:\n",
    "1. **Tokenization**: Splitting text into words, phrases, or meaningful units (tokens).\n",
    "2. **Lemmatization and Stemming**: Reducing words to their base or root form.\n",
    "   - **Example**:\n",
    "     - **Lemmatization**: \"running\" → \"run\"\n",
    "     - **Stemming**: \"running\" → \"runn\"\n",
    "3. **Part-of-Speech (POS) Tagging**: Assigning a grammatical category to each word (e.g., noun, verb, adjective).\n",
    "4. **Spell Checking**: Identifying and correcting misspelled words.\n",
    "5. **Word Recognition**: Differentiating valid words from non-words.\n",
    "\n",
    "### Example:\n",
    "Consider the sentence: `\"Cats are running.\"`\n",
    "- **Lexical Processing identifies**:\n",
    "  - \"Cats\" as a plural noun\n",
    "  - \"are\" as an auxiliary verb\n",
    "  - \"running\" as a verb (present participle)\n",
    "\n",
    "### Code Example (Tokenization, Lemmatization, Stemming, POS Tagging):\n",
    "```python\n",
    "# Importing necessary libraries for lexical processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Example Sentence\n",
    "sentence = \"Cats are running around the park.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "\n",
    "# Part-of-Speech (POS) Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Syntactic  Processing\n",
    "\n",
    "#### Description:\n",
    "Syntactic processing focuses on the structure of sentences, ensuring that the arrangement of words follows the rules of grammar. It involves analyzing the grammatical structure of a sentence to check if it is syntactically valid.\n",
    "\n",
    "#### Key Tasks:\n",
    "1. **Parsing**:\n",
    "    - Analyzing sentences to identify their grammatical structure.\n",
    "2. **Parse Tree**:\n",
    "    - A hierarchical tree structure that represents the syntactic structure of a sentence.\n",
    "3.  **Dependency Parsing**:\n",
    "    - Identifying dependencies between words in a sentence (e.g., subject-verb relationships).\n",
    "4. **Syntax Error Detection**:\n",
    "    - Identifying incorrect grammatical structures.\n",
    "\n",
    "- **Example**:\n",
    "    -   Sentence: \"He goes to park\"\n",
    "        -   Error: Missing \"the\" before \"park\" (should be \"the park\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Example (Parse Tree Generation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m grammar \u001b[38;5;241m=\u001b[39m CFG\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m  S -> NP VP\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m  NP -> Det N\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m  V -> \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mran\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create a parser\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mChartParser(grammar)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Parse a sentence\u001b[39;00m\n\u001b[0;32m     17\u001b[0m sentence_parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe cat sat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a simple grammar for parsing\n",
    "from nltk import CFG\n",
    "\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det N\n",
    "  VP -> V NP\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'cat' | 'mat'\n",
    "  V -> 'sat' | 'ran'\n",
    "\"\"\")\n",
    "\n",
    "# Create a parser\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# Parse a sentence\n",
    "sentence_parse = 'the cat sat'.split()\n",
    "for tree in parser.parse(sentence_parse):\n",
    "    print(tree)\n",
    "    tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Semantic Processing:\n",
    "\n",
    "#### Description:\n",
    "Semantic processing deals with the meaning of words and sentences. It extracts and represents the meaning of the text, resolving ambiguities and capturing relationships between concepts\n",
    "\n",
    "#### Key Tasks:\n",
    "1. **Word Sense Disambiguation (WSD)**: Identifying the correct meaning of a word based on context.\n",
    "- **Example**:\n",
    "    -   \"bank\" → Financial institution (in \"He went to the bank\").\n",
    "    -   \"bank\" → Riverbank (in \"He sat by the bank\").\n",
    "2. **Named Entity Recognition (NER)**: Identifying proper nouns and their categories (e.g., names, places).\n",
    "- **Example**:\n",
    "    -   \"Apple\" → Organization\n",
    "    -   \"Paris\" → Location\n",
    "3. **Semantic Role Labeling (SRL)**: Identifying roles of words in a sentence (e.g., subject, object).\n",
    "- **Example**:\n",
    "    -   Sentence: \"John gave Mary a gift.\"\n",
    "    -   Roles: John = giver, Mary = receiver, gift = object.\n",
    "4. **Coreference Resolution**: Linking pronouns and phrases to their referents.\n",
    "- **Example**:\n",
    "    -   Sentence: \"The cat sat on the mat. It was fluffy.\"\n",
    "    -   \"It\" refers to \"The cat.\"\n",
    "5. **Relationship Extraction**: Identifying relationships between entities.\n",
    "- **Example**:\n",
    "    -   Sentence: \"Barack Obama was born in Hawaii.\"\n",
    "    -   Extracted relationship: (Barack Obama, born in, Hawaii).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Example (Named Entity Recognition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manikanta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\manikanta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\manikanta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manikanta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manikanta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\manikanta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m sentence_ner \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple is looking at buying U.K. startup for $1 billion\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m ner_tree \u001b[38;5;241m=\u001b[39m ne_chunk(\u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mpos_tag(word_tokenize(sentence_ner)))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamed Entity Recognition:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(ner_tree)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence_ner = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "ner_tree = ne_chunk(nltk.pos_tag(word_tokenize(sentence_ner)))\n",
    "print(\"Named Entity Recognition:\")\n",
    "print(ner_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unicode Standards:\n",
    "\n",
    "#### Description:\n",
    "Unicode is a universal character encoding standard that represents text from most of the world's writing systems. It allows for the encoding of text in a machine-readable format (binary data) that can be stored, transmitted, or processed.\n",
    "\n",
    "#### How Encoding Works After Linguistic Processing:\n",
    "1. **Input Text**: \n",
    "    -   Processed text (e.g., tokens, syntactic structures) is prepared for encoding.\n",
    "    - **Example**:  \n",
    "        -   \"John gave a book to Mary.\"\n",
    "2. **Unicode Mapping**: \n",
    "    -   Each character is mapped to a unique Unicode code point.\n",
    "    - **Example**:  \n",
    "        - \"J\" → U+004A\n",
    "        - \"o\" → U+006F\n",
    "        - \" \" (space) → U+0020\n",
    "3. **Encoding Format**: \n",
    "    - The Unicode code points are converted into a specific encoding format, such as:\n",
    "        - **UTF-8**: Variable-length encoding; widely used.\n",
    "        - **UTF-16**: Fixed-length or variable-length encoding; supports more complex scripts.\n",
    "        - **UTF-32**: Fixed-length encoding; uses 4 bytes for every character.\n",
    "4. **Output**: \n",
    "    - Encoded text ready for storage or transmission.\n",
    "    - **Example**: (UTF-8 encoding for \"John\")\n",
    "        - J → 01001010\n",
    "        - o → 01101111\n",
    "        - h → 01101000\n",
    "        - n → 01101110\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Example (Encoding and Decoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default string: ₹50 \n",
      " Type of string: <class 'str'> \n",
      "\n",
      "Encoded to UTF-8: b'\\xe2\\x82\\xb950' \n",
      " Type of string: <class 'bytes'> \n",
      "\n",
      "Decoded from UTF-8: ₹50 \n",
      " Type of string: <class 'str'> \n",
      "\n",
      "\n",
      "Encoding formats example:\n",
      "UTF-8 encoding for 'John': b'John'\n",
      "UTF-16 encoding for 'John': b'\\xff\\xfeJ\\x00o\\x00h\\x00n\\x00'\n",
      "UTF-32 encoding for 'John': b'\\xff\\xfe\\x00\\x00J\\x00\\x00\\x00o\\x00\\x00\\x00h\\x00\\x00\\x00n\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating how text encoding works\n",
    "amount = u\"₹50\"\n",
    "print('Default string:', amount, '\\n', 'Type of string:', type(amount), '\\n')\n",
    "\n",
    "# Encode to UTF-8 byte format\n",
    "amount_encoded = amount.encode('utf-8')\n",
    "print('Encoded to UTF-8:', amount_encoded, '\\n', 'Type of string:', type(amount_encoded), '\\n')\n",
    "\n",
    "# Decode from UTF-8 byte format\n",
    "amount_decoded = amount_encoded.decode('utf-8')\n",
    "print('Decoded from UTF-8:', amount_decoded, '\\n', 'Type of string:', type(amount_decoded), '\\n')\n",
    "\n",
    "# Encoding format examples\n",
    "print(\"\\nEncoding formats example:\")\n",
    "utf8_example = \"John\"\n",
    "utf16_example = \"John\"\n",
    "utf32_example = \"John\"\n",
    "\n",
    "print(f\"UTF-8 encoding for 'John': {utf8_example.encode('utf-8')}\")\n",
    "print(f\"UTF-16 encoding for 'John': {utf16_example.encode('utf-16')}\")\n",
    "print(f\"UTF-32 encoding for 'John': {utf32_example.encode('utf-32')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
