{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471ae171-5855-479e-8ecf-831ebc64088f",
   "metadata": {},
   "source": [
    "# Viterbi Heuristic\n",
    "\n",
    "## Introduction\n",
    "- The **Viterbi Heuristic** is a simplified version of the **Viterbi Algorithm**, which is used to find the most probable sequence of hidden states in a **Hidden Markov Model (HMM)**. \n",
    "- It is typically used in tasks like **part-of-speech tagging**, **speech recognition**, and **machine translation**.\n",
    "\n",
    "## Key Components of HMM\n",
    "1. **States**: Hidden states (e.g., parts of speech, phonemes).\n",
    "2. **Observations**: Observable outputs corresponding to each state (e.g., words, acoustic features).\n",
    "3. **Transition Probabilities**: Probabilities of moving from one state to another.\n",
    "4. **Emission Probabilities**: Probabilities of observing a particular output from a state.\n",
    "\n",
    "## Viterbi Algorithm\n",
    "The Viterbi algorithm computes the most probable sequence of hidden states for a given sequence of observations.\n",
    "\n",
    "### Steps:\n",
    "1. **Initialization**: Calculate the probability of starting in each state for the first observation.\n",
    "2. **Recursion**: For each subsequent observation, calculate the probability of arriving at each state from every possible previous state.\n",
    "3. **Termination**: Once the last observation is processed, backtrack to find the best state sequence.\n",
    "\n",
    "## Viterbi Heuristic\n",
    "The **Viterbi Heuristic** is a simplified or approximate version of the Viterbi algorithm. It:\n",
    "- **Limits the search space** to likely states.\n",
    "- **Prunes unlikely paths**.\n",
    "- **Uses approximation techniques** to speed up the computation.\n",
    "\n",
    "### Common Techniques:\n",
    "- **Pruning**: Discarding unlikely paths early on.\n",
    "- **Greedy Decisions**: Choosing the most probable state at each step.\n",
    "- **Beam Search**: Keeping the top **k** most likely paths at each step.\n",
    "\n",
    "## Example Use Case\n",
    "### Part-of-Speech Tagging:\n",
    "- Input: \"The cat sleeps.\"\n",
    "- Hidden states: \"DT\" (Determiner), \"NN\" (Noun), \"VBZ\" (Verb).\n",
    "- The Viterbi algorithm finds the most likely sequence of tags for this sentence. The heuristic might simplify by:\n",
    "  - Keeping only the top **k** most likely sequences.\n",
    "  - Pruning unlikely tag transitions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ea34b-6d74-40b0-891f-9c3aa031be2b",
   "metadata": {},
   "source": [
    "# Understanding Transition and Emission Probabilities for Part-of-Speech Tagging\n",
    "\n",
    "## 1. Transition Probabilities:\n",
    "Transition probabilities represent the likelihood of transitioning from one state to another. \n",
    "\n",
    "- **Example in POS tagging**: \n",
    "  - It tells you how likely it is for a word tagged as **\"Determiner\" (DT)** to be followed by a word tagged as **\"Noun\" (NN)**, or for a verb to follow a noun, etc.\n",
    "  - For instance, we might expect the probability of going from **DT → NN** to be high, while **NN → VBZ** might be less likely.\n",
    "\n",
    "- **Why We Need Transition Probabilities**:\n",
    "  - Without transition probabilities, the algorithm wouldn’t have a basis to decide the most probable state transitions between consecutive words in the sequence.\n",
    "  \n",
    "## 2. Emission Probabilities:\n",
    "Emission probabilities represent the likelihood of a given observation (word) being generated from a particular state (POS tag). \n",
    "\n",
    "- **Example in POS tagging**: \n",
    "  - For example, the word **\"cat\"** is most likely to be tagged as a **Noun (NN)**, and **\"sleeps\"** is most likely to be tagged as a **Verb (VBZ)**.\n",
    "  - Emission probabilities specify how likely it is that a specific word corresponds to a particular POS tag.\n",
    "\n",
    "- **Why We Need Emission Probabilities**:\n",
    "  - Without emission probabilities, the algorithm wouldn’t be able to assess the likelihood of a word belonging to a specific POS tag. This is crucial for determining the best tag sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why You Need Both Transition and Emission Probabilities in POS Tagging:**\n",
    "\n",
    "- **Transition probabilities** allow the algorithm to choose the most probable path of states (tags in the case of POS tagging).\n",
    "  - For example, the model might evaluate the likelihood of a sequence like **DT → NN → VBZ**.\n",
    "\n",
    "- **Emission probabilities** allow the algorithm to calculate the likelihood of observing specific words given a state.\n",
    "  - For example, the model might evaluate the likelihood of the word **\"cat\"** being tagged as **NN**.\n",
    "\n",
    "#### **Example**:\n",
    "\n",
    "For a sentence like **\"The cat sleeps\"**, you’d need both transition and emission probabilities for the model to:\n",
    "\n",
    "- Estimate the likelihood of transitions between POS tags:\n",
    "  - E.g., how likely is it that a word tagged as **DT** (Determiner) is followed by a word tagged as **NN** (Noun), and how likely is it that a **NN** is followed by a **VBZ** (Verb, 3rd person singular)?\n",
    "  \n",
    "- Estimate the likelihood of each word being associated with a specific POS tag:\n",
    "  - E.g., how likely is the word **\"cat\"** to be tagged as **NN** and **\"sleeps\"** to be tagged as **VBZ**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Defining Probabilities**:\n",
    "\n",
    "In practice, these probabilities are either:\n",
    "\n",
    "1. **Manually Defined**:\n",
    "   - Based on linguistic knowledge or a corpus of labeled data.\n",
    "   - For small illustrative examples, these probabilities might be defined manually or with assumptions.\n",
    "\n",
    "2. **Learned from Data**:\n",
    "   - In real-world applications, these probabilities are **learned** by training a model on a labeled dataset (e.g., a large text corpus where words are already tagged with POS tags).\n",
    "   - For example, using datasets like the **Penn Treebank** or similar corpora, transition and emission probabilities can be computed from the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of a Realistic Setup**:\n",
    "\n",
    "1. **Transition Probability**: \\( P(\\text{tag}_t \\mid \\text{tag}_{t-1}) \\)\n",
    "   - Example: How likely is it that a word tagged as **NN** (Noun) is followed by a word tagged as **VBZ** (Verb, 3rd person singular)?\n",
    "\n",
    "2. **Emission Probability**: \\( P(\\text{word}_t \\mid \\text{tag}_t) \\)\n",
    "   - Example: How likely is the word **\"cat\"** to be tagged as **NN**?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7b2cf9-f05e-47fb-a770-82b1f48ddb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best POS tag sequence: ['DT', 'NN', 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the transition and emission probabilities as dictionaries\n",
    "transition_probs = {\n",
    "    'DT': {'DT': 0.1, 'NN': 0.7, 'VBZ': 0.2},\n",
    "    'NN': {'DT': 0.6, 'NN': 0.3, 'VBZ': 0.1},\n",
    "    'VBZ': {'DT': 0.2, 'NN': 0.3, 'VBZ': 0.5}\n",
    "}\n",
    "\n",
    "emission_probs = {\n",
    "    'DT': {'The': 0.9, 'cat': 0.1, 'sleeps': 0.05},\n",
    "    'NN': {'The': 0.05, 'cat': 0.8, 'sleeps': 0.05},\n",
    "    'VBZ': {'The': 0.05, 'cat': 0.1, 'sleeps': 0.9}\n",
    "}\n",
    "\n",
    "# Observations (words in the sentence)\n",
    "observations = ['The', 'cat', 'sleeps']\n",
    "\n",
    "# Hidden states (POS tags)\n",
    "states = ['DT', 'NN', 'VBZ']\n",
    "\n",
    "# Initialize the Viterbi matrix (stores the highest probabilities)\n",
    "viterbi = {state: [0] * len(observations) for state in states}\n",
    "\n",
    "# Initialize the backpointer matrix (for tracking the best path)\n",
    "backpointer = {state: [None] * len(observations) for state in states}\n",
    "\n",
    "# Step 1: Initialization (first observation)\n",
    "for state in states:\n",
    "    viterbi[state][0] = emission_probs[state].get(observations[0], 0) * 1  # Initial probability (P(start) = 1)\n",
    "\n",
    "# Step 2: Recursion (for subsequent observations)\n",
    "k = 2  # Keep top k most probable states\n",
    "for t in range(1, len(observations)):\n",
    "    word = observations[t]\n",
    "    \n",
    "    for current_state in states:\n",
    "        state_probabilities = []\n",
    "        \n",
    "        for previous_state in states:\n",
    "            transition_prob = transition_probs[previous_state].get(current_state, 0)\n",
    "            emission_prob = emission_probs[current_state].get(word, 0)\n",
    "            prob = viterbi[previous_state][t-1] * transition_prob * emission_prob\n",
    "            state_probabilities.append((prob, previous_state))\n",
    "        \n",
    "        # Sort and keep the top k states\n",
    "        state_probabilities.sort(reverse=True, key=lambda x: x[0])\n",
    "        top_k_states = state_probabilities[:k]\n",
    "        \n",
    "        # Store the most probable path\n",
    "        best_prob, best_state = top_k_states[0]\n",
    "        viterbi[current_state][t] = best_prob\n",
    "        backpointer[current_state][t] = best_state\n",
    "\n",
    "# Step 3: Termination (backtrack to find the best path)\n",
    "# Start from the final time step (last word in the observations)\n",
    "best_final_state = max(viterbi, key=lambda state: viterbi[state][len(observations)-1])\n",
    "best_path = [best_final_state]\n",
    "\n",
    "# Backtrack to find the most probable state sequence\n",
    "for t in range(len(observations)-1, 0, -1):\n",
    "    best_final_state = backpointer[best_final_state][t]\n",
    "    best_path.insert(0, best_final_state)\n",
    "\n",
    "# Output the best path (POS tags sequence)\n",
    "print(\"Best POS tag sequence:\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965219ca-9367-459d-b0c7-3599f872acd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
