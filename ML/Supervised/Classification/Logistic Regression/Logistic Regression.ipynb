{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4873d170",
   "metadata": {},
   "source": [
    "\n",
    "# Logistic Regression\n",
    "\n",
    "### 1. Overview\n",
    "\n",
    "- **Type:** Supervised machine learning algorithm used for **classification** problems.\n",
    "- Unlike linear regression which predicts continuous values, logistic regression predicts the **probability** that an example belongs to a particular class (usually binary: 0 or 1).\n",
    "- Generalizes to multiclass problems via extensions.\n",
    "\n",
    "\n",
    "### 2. Purpose and Use Cases\n",
    "\n",
    "- Predicts categories such as:\n",
    "    - Yes/No (binary classification)\n",
    "    - Success/Failure\n",
    "    - Spam/Not Spam\n",
    "- Widely used in medical diagnosis, credit risk assessment, marketing, etc.\n",
    "\n",
    "\n",
    "### 3. Core Mathematical Concepts\n",
    "\n",
    "#### Logistic Function (Sigmoid)\n",
    "\n",
    "- Transforms any real-valued number into output between 0 and 1.\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "- \\$ \\beta_0 \\$ is the intercept (bias), \\$ \\beta_i \\$ are coefficients.\n",
    "- Output \\$ \\sigma(z) \\$ represents \\$ P(y=1|x) \\$, the probability input \\$ x \\$ belongs to class 1.\n",
    "\n",
    "\n",
    "### 4. Model Equation\n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\sum_{i=1}^n \\beta_i x_i)}}\n",
    "$$\n",
    "\n",
    "- $P(y=0|x) = 1 - P(y=1|x)$\n",
    "- To predict class, typically apply a threshold:\n",
    "    - Predict 1 if $P(y=1|x) \\geq 0.5$\n",
    "    - Predict 0 otherwise\n",
    "\n",
    "\n",
    "### 5. Likelihood and Loss Function\n",
    "\n",
    "- Logistic regression parameters \\$ \\beta \\$ are found by **maximizing the likelihood** of observed data.\n",
    "- Likelihood function for binary outcomes:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n P(y_i|x_i) = \\prod_{i=1}^n [p(x_i)]^{y_i} [1 - p(x_i)]^{1-y_i}\n",
    "$$\n",
    "\n",
    "- More commonly, optimize the **log-likelihood** (sum of logs for numerical stability):\n",
    "\n",
    "$$\n",
    "\\log L(\\beta) = \\sum_{i=1}^n \\left[y_i \\log p(x_i) + (1 - y_i) \\log(1 - p(x_i))\\right]\n",
    "$$\n",
    "\n",
    "- Equivalent to minimizing the **binary cross-entropy loss** (log loss).\n",
    "\n",
    "\n",
    "### 6. Training Algorithm\n",
    "\n",
    "- Typically trained using **maximum likelihood estimation**.\n",
    "- Optimization done with **gradient descent** or its variants (stochastic gradient descent, batch gradient descent).\n",
    "- Updates parameters iteratively to minimize the log loss.\n",
    "\n",
    "\n",
    "### 7. Model Interpretation\n",
    "\n",
    "- Coefficients \\$ \\beta_i \\$ represent the **log-odds** change in the outcome for a unit increase in feature \\$ x_i \\$.\n",
    "- Exponentiating a coefficient yields the **odds ratio** for that feature.\n",
    "- The intercept \\$ \\beta_0 \\$ is the log-odds of the outcome when all features are 0.\n",
    "\n",
    "\n",
    "### 8. Assumptions\n",
    "\n",
    "- The log-odds of the outcome is a linear combination of features.\n",
    "- Observations are independent.\n",
    "- No exact multicollinearity among features.\n",
    "- Large sample sizes generally needed for stable estimates.\n",
    "\n",
    "\n",
    "### 9. Evaluation Metrics\n",
    "\n",
    "| Metric | Description | Notes |\n",
    "| :-- | :-- | :-- |\n",
    "| Accuracy | % of correct predictions | Simple, but can be misleading with imbalanced data |\n",
    "| Precision | True Positives / (True Positives + False Positives) | Measures relevance of positive predictions |\n",
    "| Recall (Sensitivity) | True Positives / (True Positives + False Negatives) | Measures ability to detect positives |\n",
    "| F1-Score | Harmonic mean of precision \\& recall | Balanced metric for imbalanced datasets |\n",
    "| Confusion Matrix | Table of TP, FP, TN, FN | Detailed error analysis |\n",
    "| ROC Curve \\& AUC | Plots true positive rate vs false positive rate | Overall measure of model discrimination ability |\n",
    "\n",
    "### 10. Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :-- | :-- |\n",
    "| Simple, interpretable, and fast to train | Assumes linear relationship in log-odds |\n",
    "| Outputs calibrated probabilities | Can struggle with complex, non-linear data |\n",
    "| Works well for linearly separable data | Sensitive to outliers and multicollinearity |\n",
    "| Foundation for more advanced classification methods | Requires careful feature engineering |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38306bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
